{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最近邻方法kNN\n",
    "\n",
    "即搜索样本点最近的k个邻居,以决定其分类,是一种懒惰学习的算法.\n",
    "\n",
    "## K-means算法\n",
    "\n",
    "\n",
    "## 决策树\n",
    "决策树简单的理解为if-then的集合，其优点主要是分类速度快、可读性等。决策树的生成主要可分为三个步骤：特征的选择、决策树的生成、决策树的剪纸。\n",
    "### 特征选择\n",
    "对于结点的选择，总得需要一个计算方法来实现，这个方法的目标是优先选择分类能力强的特征，这样才提高决策树的效率，如果随机选择特征的话将会产生复杂度或者是结点更多的决策树，显然不是我们想要的。  \n",
    "怎么计算特征的分类能力呢，通常选择的准则是信息增益或信息增益比。  要计算信息增益，首先要知道熵的概念：表示随机变量不确定程度，设X是一个取有限个值的离散随机变量，其概率分布为：  \n",
    "$$P(X = x_i) = P_i,i = 1,2,\\dots,n$$\n",
    "则随机变量的熵定义为：\n",
    "$$H(X) = - \\sum^n_{i=1}p_i\\log_2p_i$$\n",
    "条件熵H(Y|X),其定义为X给定的条件下Y的条件概率分布的熵对X的数学期望：\n",
    "$$H(X|Y) = \\sum^n{i=1}p_iH(Y|X = x_i)$$\n",
    "信息增益表示得知特征X的信息而使类Y的不确定性减少的程度。定义为： \n",
    "$$g(D,A) = H(D) - H(D|A)$$\n",
    "信息增益比：信息增益g（D,A)与训练数据集D关于A的值的熵之比，即 \n",
    "$$g_R(D,A) =\\frac{g(D,A)}{H_A(D)} $$\n",
    "### 生成决策树\n",
    "对生成树，ID3算法的方法是从根结点开始，对结点计算所有可能的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点，再递归的计算信息增益建立结点，直到信息增益均很小或者没有特征可以选择为止.C4.5算法，与ID3算法唯一不同是使用信息增益比来选择特征。\n",
    "### 剪枝\n",
    "当使用训练数据进行生成决策树，而不进行处理时，会出现训练集精度为100%，但测试集上表现不是很好，这是由于对训练集进行生成决策树时过于细分，不进行剪枝，很有可能产生过拟合。剪枝分为两种，1、预剪枝，在构造树的同时停止信息量较少，也就是信息增益或信息增益比较小的枝生长。2、后剪枝，先生成树，再进行剪枝。\n",
    "\n",
    "## 朴素贝叶斯方法\n",
    "\n",
    "## 逻辑斯蒂回归\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集的使用与可视化\n",
    "\n",
    "我们采用了广为流传的iris数据集,并且sklearn的datasets模块就为我们提供了这一数据集."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "1) datasets.load_<dataset_name>：sklearn包自带的小数据集\n",
    "2) datasets.fetch_<dataset_name>：比较大的数据集，支持在线下载\n",
    "3) datasets.make_<dataset_name>：构造数据集\n",
    "\"\"\"\n",
    "\n",
    "data = datasets.load_iris() # 加载 IRIS 数据集\n",
    "print('keys: \\n\\t', data.keys()) # ['data', 'target', 'target_names', 'DESCR', 'feature_names']\n",
    "feature_names = data.get('feature_names')\n",
    "\n",
    "print('feature names: \\n\\t', data.get('feature_names')) # 查看属性名称\n",
    "print('target names: \\n\\t', data.get('target_names')) # 查看 label 名称\n",
    "x = data.get('data') # 获取样本矩阵\n",
    "y = data.get('target') # 获取与样本对应的 label 向量\n",
    "\n",
    "print(data.get('DESCR'))\n",
    "\n",
    "f = []\n",
    "f.append(y==0) # 类别为第一类的样本的逻辑索引\n",
    "f.append(y==1) # 类别为第二类的样本的逻辑索引\n",
    "f.append(y==2) # 类别为第三类的样本的逻辑索引\n",
    "\n",
    "color = ['red','blue','green']\n",
    "fig, axes = plt.subplots(4,4) # 绘制四个属性两辆之间的散点图\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    row  = i // 4\n",
    "    col = i % 4\n",
    "    if row == col:\n",
    "        ax.text(.1,.5, feature_names[row][0:-5]+'\\n  (cm)', size=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        continue\n",
    "    for  k in range(3):\n",
    "        ax.scatter(x[f[k],row], x[f[k],col], c=color[k], s=3)    \n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.3) # 设置间距\n",
    "plt.savefig('Iris_visual.png')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "keys: \n",
    "\t dict_keys(['feature_names', 'target_names', 'DESCR', 'target', 'data'])\n",
    "feature names: \n",
    "\t ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "target names: \n",
    "\t ['setosa' 'versicolor' 'virginica']\n",
    "Iris Plants Database\n",
    "====================\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Data Set Characteristics:\n",
    "    :Number of Instances: 150 (50 in each of three classes)\n",
    "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "    :Attribute Information:\n",
    "        - sepal length in cm\n",
    "        - sepal width in cm\n",
    "        - petal length in cm\n",
    "        - petal width in cm\n",
    "        - class:\n",
    "                - Iris-Setosa\n",
    "                - Iris-Versicolour\n",
    "                - Iris-Virginica\n",
    "    :Summary Statistics:\n",
    "\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "                    Min  Max   Mean    SD   Class Correlation\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
    "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
    "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
    "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "    :Class Distribution: 33.3% for each of 3 classes.\n",
    "    :Creator: R.A. Fisher\n",
    "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
    "    :Date: July, 1988\n",
    "\n",
    "This is a copy of UCI ML iris datasets.\n",
    "http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "The famous Iris database, first used by Sir R.A Fisher\n",
    "\n",
    "This is perhaps the best known database to be found in the\n",
    "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
    "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
    "data set contains 3 classes of 50 instances each, where each class refers to a\n",
    "type of iris plant.  One class is linearly separable from the other 2; the\n",
    "latter are NOT linearly separable from each other.\n",
    "\n",
    "References\n",
    "----------\n",
    "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
    "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
    "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
    "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
    "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
    "     Structure and Classification Rule for Recognition in Partially Exposed\n",
    "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
    "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
    "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
    "     on Information Theory, May 1972, 431-433.\n",
    "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
    "     conceptual clustering system finds 3 classes in the data.\n",
    "   - Many, many more ...\n",
    "```\n",
    "\n",
    "![iris](./Iris_visual.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fitting error</th>\n",
       "      <th>test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>k-NearestNeighbor</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k-means</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.964444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision tree</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.964762</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.958095</td>\n",
       "      <td>0.948889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fitting error  test error\n",
       "k-NearestNeighbor         0.976190    0.946667\n",
       "k-means                   0.971429    0.964444\n",
       "Decision tree             1.000000    0.933333\n",
       "Naive Bayes               0.964762    0.946667\n",
       "Logistic Regression       0.958095    0.948889"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\"\"\"\n",
    "1) datasets.load_<dataset_name>：sklearn包自带的小数据集\n",
    "2) datasets.fetch_<dataset_name>：比较大的数据集，支持在线下载\n",
    "3) datasets.make_<dataset_name>：构造数据集\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN模块\n",
    "from sklearn.tree import DecisionTreeClassifier # 决策树模块\n",
    "from sklearn.cluster import KMeans # kmean聚类模型\n",
    "from sklearn import linear_model # 线性逻辑斯蒂回归\n",
    "from sklearn.naive_bayes import GaussianNB # 朴素贝叶斯\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "ds = datasets.load_iris()\n",
    "X = ds[\"data\"]\n",
    "y = ds[\"target\"]\n",
    "\n",
    "Ntime = 10\n",
    "accur = np.zeros((5,2))\n",
    "for itime in range(Ntime):\n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    # kNN, kmeans, decision tree, naive bayes, logistic regression\n",
    "    clf_kNN = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    Ncluster = 10; \n",
    "    clf_kmeans = KMeans(n_clusters = Ncluster, init='k-means++').fit(X_train)\n",
    "    clf_DTree = DecisionTreeClassifier().fit(X_train, y_train) # 建立决策树对象\n",
    "    clf_NBayes = GaussianNB().fit(X_train, y_train)\n",
    "    clf_LogReg = linear_model.LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "    #print(clf_kmeans.cluster_centers_, clf_kmeans.labels_)\n",
    "    clustermap = np.zeros((Ncluster))\n",
    "    for i in range(Ncluster):\n",
    "        icluster_idx = np.where(clf_kmeans.labels_==i)\n",
    "        icluster_label = y_train[icluster_idx]\n",
    "        clustermap[i] = np.argmax(np.bincount(icluster_label))\n",
    "    #print(clustermap)\n",
    "\n",
    "    def kmeans_score(clf, X_t, y_t):\n",
    "        pre_label = clf.predict(X_t)\n",
    "        y_pre = clustermap[pre_label]\n",
    "        return sum(y_pre==y_t)/len(y_t)\n",
    "    \n",
    "    accur[0,0] += clf_kNN.score(X_train, y_train)\n",
    "    accur[1,0] += kmeans_score(clf_kmeans,X_train, y_train)\n",
    "    accur[2,0] += clf_DTree.score(X_train, y_train)\n",
    "    accur[3,0] += clf_NBayes.score(X_train, y_train)\n",
    "    accur[4,0] += clf_LogReg.score(X_train, y_train)\n",
    "    \n",
    "    accur[0,1] += clf_kNN.score(X_test, y_test)\n",
    "    accur[1,1] += kmeans_score(clf_kmeans,X_test, y_test)\n",
    "    accur[2,1] += clf_DTree.score(X_test, y_test)\n",
    "    accur[3,1] += clf_NBayes.score(X_test, y_test)\n",
    "    accur[4,1] += clf_LogReg.score(X_test, y_test)\n",
    "    \n",
    "accur /= Ntime\n",
    "\n",
    "df_accur = pd.DataFrame(accur)\n",
    "df_accur.rename( index={0:'k-NearestNeighbor',1:'k-means',2:'Decision tree',3:'Naive Bayes',4:'Logistic Regression'}, \n",
    "           columns = {0:'fitting error',1:'test error'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
