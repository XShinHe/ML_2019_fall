{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集 V = {$(x_i,y_i)$}$^N_{i=1}$,$y_i \\in ${1,-1},$x_i \\in R^n$是一个二分类问题\n",
    "\n",
    "目标是找一个超平面$$w \\cdot x +b$$\n",
    "使得对于正类有$$w \\cdot x +b>0$$ \n",
    "对于负类有$$w \\cdot x +b<0$$\n",
    "误分类点$(x_i,y_i)$有$$y_i \\dot (w \\cdot x_i +b)<0$$\n",
    "使误分类点个数最少等价于函数间隔最小即$$\\gamma = min_{i=1,2,\\cdots,N} y_i \\dot (w \\cdot x_i +b)$$\n",
    "现要找使得最大间隔分离超平面$max \\frac{\\gamma}{||w||}$使得$y_i \\dot (\\frac{w}{||w||} \\cdot x_i + \\frac{b}{||w||})\\geq \\frac{\\gamma}{||w||}$,i = 1,2$\\cdots$N  \n",
    "等价于 min$\\frac{1}{2} \\dot {||w||}^2$使得$y_i \\dot (w \\cdot x_i +b)-1\\geq0$,i = 1,2$\\cdots$N  \n",
    "现引入Lagrange函数,$$L(w,b,\\alpha) = \\frac{1}{2} \\dot {||w||}^2 - \\sum ^N_{i=1} \\alpha_i\\dot y_i\\dot (w \\cdot x_i +b)+\\sum ^N_{i=1} \\alpha_i$$\n",
    "求解其对偶问题$$max_\\alpha min_{w,b}L(w,b,\\alpha)$$\n",
    "$$y_i \\dot (w \\cdot x_i +b)-1\\geq 0$$  \n",
    "求解$min_{w,b}L(w,b,\\alpha)$,将Lagrange分别对w,b求偏导数并令其等于0，得  \n",
    "$$w = \\sum ^N_{i=1} \\alpha_i\\dot y_i\\dot x_i$$\n",
    "$$\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$  \n",
    "回带即得$$min_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)+\\sum ^N_{i=1} \\alpha_i$$  \n",
    "即求得等价对偶问题$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i$$  \n",
    "$$\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$ \n",
    "$$\\alpha_i\\geq0,i=1,2\\cdots,N$$\n",
    "满足KKT条件，记对偶问题最优解$(\\alpha^*,w^*,b^*)$，则有:  \n",
    "$$\\alpha^*_i\\dot y_i \\dot (w \\cdot x_i +b)-1 = 0,i = 1,2\\cdots,N$$\n",
    "$$y_i \\dot (w \\cdot x_i +b)-1\\geq0,i = 1,2\\cdots,N$$\n",
    "$$\\alpha^*_i\\geq0,i = 1,2\\cdots,N$$\n",
    "$\\Rightarrow \\alpha^*_i = 0,y_i \\dot (w \\cdot x_i +b)-1 > 0$，此时$\\alpha^*_i$不为支持向量  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha^*_i > 0,y_i \\dot (w \\cdot x_i +b)-1 = 0$，此时$\\alpha^*_i$为支持向量   \n",
    "所以支持向量的个数即$\\alpha^*_i > 0$的个数，为了有效减少支持向量的个数，可对对偶问题进行优化，使得$=\\alpha^*_i > 0$的个数减少.故在等价对偶问题对其加入惩罚项$B\\dot\\sum ^N_{i=1} \\alpha_i$,其中的B非负，得$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i+B\\dot\\sum ^N_{i=1} \\alpha_i$$  约化得:$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)+(B-1)\\dot\\sum ^N_{i=1} \\alpha_i$$  \n",
    "现实生活中训练数据线性可分的情形较少，训练数据往往是近似线性可分的，此时需要引入松弛变量$\\xi_i$及惩罚因子C>0，使其可分，此时原始最优化问题变为$$min_{w,b,\\xi}\\frac{1}{2} \\dot {||w||}^2+C\\dot\\sum^N_{i=1}\\xi_i$$使得$$y_i \\dot (w \\cdot x_i +b)\\geq 1-\\xi_i,i=1,2\\cdots,N$$$$\\xi_i\\geq0,i=1,2\\cdots,N$$  \n",
    "求解其等价对偶问题$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i$$使得$$\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$$$0\\leq\\alpha_i\\leq C,i = 1,2\\cdots N$$将其解$\\alpha^*$中$\\alpha^*\\>0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量，所以支持向量的个数即$\\alpha^*_i > 0$的个数，同理为了有效减少支持向量的个数，可对对偶问题进行优化，使得$=\\alpha^*_i > 0$的个数减少.故在等价对偶问题对其加入惩罚项$B\\dot\\sum ^N_{i=1} \\alpha_i$,其中的B非负，得$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i+B\\dot\\sum ^N_{i=1} \\alpha_i$$  约化得:$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)+(B-1)\\dot\\sum ^N_{i=1} \\alpha_i$$使得$$\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$$$0\\leq\\alpha_i\\leq C,i = 1,2\\cdots N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**已检查出的错误地方:**  \n",
    "123与125行np.norm改成np.linalg.norm 理由没有上述模块  \n",
    "109与128行的get语法用错，从空类型里得不到(我是直接赋值的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 1.000000\n",
      "\n",
      "Hyper surface:\n",
      "\tw =\n",
      " [2.67674897 0.74171251] \n",
      "\tb =\n",
      " -1.0846022142621072\n",
      "\n",
      "Support vector number:\t4\n",
      "\n",
      "Support vector f(x):\n",
      " [-1.         -0.20738678  0.51211796  1.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"@package docstring\n",
    "Support Vector Machine: ML teamwork\n",
    "@ author:\n",
    "    1) Xin He\n",
    "\"\"\"\n",
    "\n",
    "class svm:\n",
    "    \"\"\"Support Vector Machine:\n",
    "        This is a basic SVM class, with some enhenced algorithm to reduce the number of support vectors\n",
    "        some references:\n",
    "        1) [Ho Gi Jung, and Gahyun Kim, Support Vector Number Reduction: Survey and Experimental \n",
    "        Evaluations, IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL.15, NO.2, APRIL \n",
    "        2014](https://ieeexplore.ieee.org/document/6623200)\n",
    "            : On summary of different reduced methods\n",
    "        2) [P. M. L. Drezet and R. F. Harrison, “A new method for sparsity control in support vector \n",
    "        classification and regression,” Pattern Recognit., vol.34, no.1, pp. 111–125, Jan. 2001](\n",
    "        https://www.sciencedirect.com/science/article/pii/S0031320399002034)\n",
    "            : \n",
    "        3) [O. Dekel and Y. Singer, “Support vector machines on a budget,” in Proc. NIPS Found., 2006, \n",
    "        pp.1–8](https://ieeexplore.ieee.org/book/6267330)\n",
    "        4) [S.-Y. Chiu, L.-S. Lan, and Y.-C. Hwang, “Two sparsity-controlled schemes for 1-norm support \n",
    "        vector classification,” in Proc. IEEE Northeast Workshop Circuits Syst., Montreal, QC, Canada, \n",
    "        Aug. 5–8, 2007,pp. 337–340](https://ieeexplore.ieee.org/document/4487961?arnumber=4487961)\n",
    "        5) []()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        \"\"\"\n",
    "        kwargs:\n",
    "            1) select: feature selection before SVM-fit, in ['origin','random','cluster']\n",
    "            2) selectargs: dictionary arguments for select feature\n",
    "            \n",
    "            3) kernel: in ['linear', 'poly', 'gauss'], (last two should with kernalargs)\n",
    "            4) kernelargs: dictionary arguments for kernel\n",
    "            \n",
    "            5) C: penalty of unseperable data, with constraint 0 < alpha < C\n",
    "            6) gamma: penalty for support vector number\n",
    "            \n",
    "            7) postselect:\n",
    "            8) postconstruct:\n",
    "            \n",
    "            9) eps: tolerance of numerical error, default 1e-5\n",
    "            10) maxiter: max iteration times, default 2|D|\n",
    "        \"\"\"\n",
    "        \n",
    "        self.select = kwargs.get('select', 'origin')\n",
    "        self.selectargs = kwargs.get('selectargs', None)\n",
    "        \n",
    "        self.kernel = kwargs.get('kernel', 'linear')\n",
    "        self.kernelargs = kwargs.get('kernelargs', None)\n",
    "        \n",
    "        self.C = kwargs.get('C', 1)\n",
    "        self.gamma = kwargs.get('gamma', 1)\n",
    "        \n",
    "        # postselect # TODO\n",
    "        # postconstruct # TODO\n",
    "        \n",
    "        self.eps = kwargs.get('eps', 1e-10)\n",
    "        self.maxiter = kwargs.get('maxiter', 0) # laterly will auto determined as 2|D|\n",
    "        \n",
    "    def _Select(self,X,y):\n",
    "        \"\"\"\n",
    "            select feature/or sampling of data\n",
    "            1) origin\n",
    "            2) random\n",
    "            3) cluster\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.select == 'origin':\n",
    "            self.Ns = np.shape(X)[0]\n",
    "            return X,y\n",
    "        \n",
    "        elif self.select == 'random':\n",
    "            self.Ns = self.selectargs.get('N',100)\n",
    "            if self.Ns < np.shape(X)[0]:\n",
    "                Xs = np.zeros((self.Ns, np.shape(X)[1]))\n",
    "                ys = np.zeros((self.Ns))\n",
    "                idx = np.random.choice(np.shape(X)[0], self.Ns, replace=False)\n",
    "                for i in range(len(_idx)):\n",
    "                    Xs[i,:] = X[idx[i],:]\n",
    "                    ys[i] = y[idx[i]]\n",
    "                return Xs,ys\n",
    "            raise ValueError('random N exceeds oringal data')\n",
    "            \n",
    "        elif self.select == 'cluster':\n",
    "            #TODO\n",
    "            # k = self.selectargs.get('k',200)\n",
    "            # kmean = kmean(X)\n",
    "            # return kmean.kavgs\n",
    "            return X,y\n",
    "            \n",
    "    def _Kernel(self,X,Xp):\n",
    "        \"\"\"construct kernel\n",
    "            when X=Xp, are both training data, it construct an inner kenel\n",
    "            when X is training data, and Xp is a test data, it construct a crossing kernel\n",
    "        \"\"\"\n",
    "        # HERE WE USE EINSTEIN'S SUMMATION RULE FOR IMPLEMENT\n",
    "        # linear kernel: Kij = Xik Xjk\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(X, Xp.T)\n",
    "        \n",
    "        # polynormial kernel: Kij = (Xik Xjk + 1)**P\n",
    "        elif self.kernel == 'poly':\n",
    "            #P = self.kernelargs.get(2)\n",
    "            P = 2\n",
    "            return (1+np.dot(X,Xp.T))**P \n",
    "        \n",
    "        # gauss kernel: Kij = exp(-(Xik-Xjk)(Xik-Xjk)/(2*sigma**2))\n",
    "            \"\"\"@note\n",
    "                (Xik-Xjk)(Xik-Xjk) = [Xik Xik + Xjk Xjk] - 2*Xik Xjk\n",
    "            \"\"\"\n",
    "        elif self.kernel == 'gauss':\n",
    "            _N2 = np.zeros((X.shape[0]))\n",
    "            _I = np.ones((X.shape[0]))\n",
    "            _Np2 = np.zeros((Xp.shape[0]))\n",
    "            _Ip = np.ones((Xp.shape[0]))\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                _N2[i] = np.linalg.norm(X[i,:])**2\n",
    "            for i in range(Xp.shape[0]):\n",
    "                _Np2[i] = np.linalg.norm(Xp[i,:])**2\n",
    "            \n",
    "            norm2 = np.outer(_I, _Np2) + np.outer(_N2, _Ip) - 2*np.dot(X,Xp.T)\n",
    "            #sigma = self.kernelargs.get('sigma', 1 )\n",
    "            sigma = 1\n",
    "            \n",
    "            return np.exp( - norm2 / (2*sigma**2) )\n",
    "        else:\n",
    "            raise ValueError('kernel error')\n",
    "    \n",
    "    def _AlphaCor(self):\n",
    "        zero_idx = np.argwhere(self.alpha < self.eps).reshape(1,-1)[0]\n",
    "        zero_len = len(zero_idx)\n",
    "        if zero_len < self.Ns:\n",
    "            correct = np.sum(self.alpha[zero_idx])/(self.Ns - zero_len)\n",
    "            self.alpha += correct\n",
    "        self.alpha[zero_idx] = 0\n",
    "        \n",
    "        self.support_idx = np.argsort(-self.alpha)\n",
    "    \n",
    "    def _KKTBias(self):\n",
    "        \"\"\"Calculate and sort of KKT bias\n",
    "        a=0    <=>  yi (gi-yi) >= 0, first tyope\n",
    "        0<a<C  <=>  yi (gi-yi) == 0, second type\n",
    "        a=C    <=>  yi (gi-yi) <= 0, third type\n",
    "        \"\"\"\n",
    "        for i in range(self.Ns):\n",
    "            if self.alpha[i] < self.eps:\n",
    "                self.kktbias[i] = max(1-self.eps - self.y[i]*self.predict[i], 0)\n",
    "            elif self.alpha[i] > self.C - self.eps:\n",
    "                self.kktbias[i] = max(-(1+self.eps) + self.y[i]*self.predict[i], 0)\n",
    "            else:\n",
    "                self.kktbias[i] = max( abs( self.y[i]*self.predict[i] - 1 ) - self.eps, 0 )  \n",
    "        if len(np.argwhere(self.kktbias > self.eps).reshape(1,-1)[0]) > 0:\n",
    "            self.kkt_hold = False\n",
    "        else:\n",
    "            self.kkt_hold = True\n",
    "\n",
    "        self.kktbias_idx = np.argsort(-self.kktbias)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"fitting SVM: \n",
    "            train data X and label y, with in SVM\n",
    "        \"\"\"\n",
    "        \n",
    "        # sampling and determin Ns\n",
    "        self.X, self.y = self._Select(X,y)\n",
    "        if self.maxiter == 0:\n",
    "            self.maxiter = 10*self.Ns\n",
    "        \n",
    "        # inner kernel\n",
    "        self.K = self._Kernel(self.X, self.X)\n",
    "        \n",
    "        # svm training variables, initial with zero, s.t. \\sum alpha_i y_i = 0\n",
    "        self.alpha = np.zeros((self.Ns))\n",
    "        self.b = 0\n",
    "        \n",
    "        # Prediction and error\n",
    "        self.predict = np.dot(self.alpha*self.y, self.K) + self.b\n",
    "        self.error = self.predict - self.y\n",
    "        \n",
    "        # Bias from KKT condition\n",
    "        self.kktbias = np.zeros((self.Ns))\n",
    "        self.kkt_hold = False\n",
    "        self.kktbias_idx = np.zeros((self.Ns),dtype='int') # sorted indice of kktbias\n",
    "        self.support_idx = np.zeros((self.Ns),dtype='int') # sorted indice of kktbias\n",
    "        self._KKTBias() # initial calculate and sort\n",
    "        \n",
    "        #print(self.predict, self.error, self.kktbias, self.kktbias_idx)\n",
    "\n",
    "        self.SMO_sequential()\n",
    "        \n",
    "        return np.dot(self.alpha*self.y,self.X), self.b\n",
    "        \n",
    "    def SMO_sequential(self):\n",
    "        \"\"\"SMO algoithm, for choosing sequential pair (i,j) for optimization\n",
    "            heuristical idea:\n",
    "                i choose with max KKTBias\n",
    "                j choose with max |error(i) - error(j)|\n",
    "            clumsy idea: traversing the whole set\n",
    "        \"\"\"\n",
    "        nowiter = 0      \n",
    "        while nowiter < self.maxiter:\n",
    "            nowiter += 1\n",
    "            changed = False\n",
    "            \n",
    "            for i in self.kktbias_idx:\n",
    "                # find j, for maximize |Ei-Ej|\n",
    "                if(self.error[i] > 0):\n",
    "                    err_list = np.argsort(self.error) \n",
    "                else:\n",
    "                    err_list = np.argsort(-self.error)\n",
    "                \n",
    "                for j in err_list:\n",
    "                    changed = self.SMO_minimal(i,j)\n",
    "                    if changed or self.error[i] * self.error[j]>0:\n",
    "                        break\n",
    "                \n",
    "                # laterly find j from support vectors\n",
    "                if not changed:\n",
    "                    for j in self.support_idx:\n",
    "                        if j==i:\n",
    "                            continue\n",
    "                        changed = self.SMO_minimal(i,j)\n",
    "                        if changed:\n",
    "                            break\n",
    "                if changed:\n",
    "                    break\n",
    "            \n",
    "            if changed or not self.kkt_hold:\n",
    "                continue # for we must update kktbias_idx\n",
    "            else: # we use full i and j, without any change, the converged result\n",
    "                break\n",
    "        if nowiter == self.maxiter:\n",
    "            print('warning: result may not converged!')\n",
    "        \n",
    "    def SMO_minimal(self,i,j):\n",
    "        \"\"\"\n",
    "            try solve optimization problem for pair (i,j)\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(i,j)\n",
    "        \n",
    "        #print(self.kktbias)\n",
    "        #print(self.kktbias[self.kktbias_idx])\n",
    "        #print(self.kktbias[i], self.kktbias[j])\n",
    "        \n",
    "        if i==j:\n",
    "            return False\n",
    "        \n",
    "        # save old ai, aj\n",
    "        ai = self.alpha[i]\n",
    "        aj = self.alpha[j]\n",
    "        \n",
    "        L = max(0,aj-ai) if(self.y[i]!=self.y[j]) else max(0,aj+ai-self.C)\n",
    "        H = min(self.C,self.C+aj-ai) if(self.y[i]!=self.y[j]) else min(self.C,aj+ai)\n",
    "        if L >= H:\n",
    "            return False # failed\n",
    "        \n",
    "        eta = 2.0 * self.K[i,j] - self.K[i,i] - self.K[j,j] #《statistic learning method》p127(7.107)\n",
    "        if eta >= 0:\n",
    "            return False # failed, for eta should be positive\n",
    "        \n",
    "        # solve new aj\n",
    "        self.alpha[j] -= self.y[j]*(self.error[i] - self.error[j] + (1-self.gamma)*(self.y[i] - self.y[j]) ) / eta #《statistic learning method》p127(7.106)\n",
    "        self.alpha[j] = max(min(self.alpha[j],H),L) #《statistic learning method》p127(7.108)\n",
    "        \n",
    "        D_aj = self.alpha[j] - aj \n",
    "        if (abs(D_aj) < self.eps):\n",
    "            self.alpha[j] = aj # reget old value, might without reget old value?\n",
    "            return False # move not enough\n",
    "        \n",
    "        self.alpha[i] += self.y[j]*self.y[i]*(aj - self.alpha[j])#《statistic learning method》p127(7.109)\n",
    "        D_ai = self.alpha[i] - ai \n",
    "        \n",
    "        # update b and Ek\n",
    "        b1 = self.b - self.error[i] - self.y[i]*D_ai*self.K[i,i] - self.y[j]*D_aj*self.K[i,j]\n",
    "        b2 = self.b - self.error[j] - self.y[i]*D_ai*self.K[i,j] - self.y[j]*D_aj*self.K[j,j]\n",
    "        if (0 < self.alpha[i] < self.C):\n",
    "            self.b = b1\n",
    "        elif (0 < self.alpha[j] < self.C):\n",
    "            self.b = b2\n",
    "        else:\n",
    "            self.b = (b1 + b2)/2.0\n",
    "        \n",
    "        self._AlphaCor()\n",
    "        self.predict = np.dot(self.alpha*self.y, self.K) + self.b \n",
    "        self.error = self.predict - self.y\n",
    "        self._KKTBias()\n",
    "        return True\n",
    "        \n",
    "    def summary(self):\n",
    "        accuracy = self.test_accuracy(self.X,self.y)\n",
    "        w = np.dot(self.alpha,self.X)\n",
    "        svidx = np.nonzero(self.alpha)[0]\n",
    "        print('accuracy is %f'%accuracy )\n",
    "        print('\\nHyper surface:\\n\\tw =\\n',w,'\\n\\tb =\\n',self.b)\n",
    "        print('\\nSupport vector number:\\t%d'%(len(svidx)))\n",
    "        print('\\nSupport vector f(x):\\n', self.predict[svidx])\n",
    "        \n",
    "    def test_accuracy(self,Xt,yt):\n",
    "        Ns_test, _tmp = np.shape(Xt)\n",
    "        TP=0;FP=0;TN=0;FN=0\n",
    "        \n",
    "        Kt = self._Kernel(self.X,Xt)\n",
    "        f = np.dot(self.alpha*self.y, self.K) + self.b\n",
    "        TP = ( np.heaviside(np.sign(f),0)*np.heaviside(yt,0) ).sum()\n",
    "        FP = ( np.heaviside(np.sign(f),0)*np.heaviside(-yt,0) ).sum()\n",
    "        TN = ( np.heaviside(-np.sign(f),0)*np.heaviside(-yt,0) ).sum()\n",
    "        FN = ( np.heaviside(-np.sign(f),0)*np.heaviside(yt,0) ).sum()\n",
    "\n",
    "        return (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    N = 10\n",
    "    r = np.random.randint(2,size=N)\n",
    "    X = np.zeros((N,2))\n",
    "    y = np.zeros((N))\n",
    "    X[:,0] = np.random.normal(r,0.5)\n",
    "    X[:,1] = np.random.normal(r,0.5)\n",
    "    y = 2*r-1\n",
    "\n",
    "    \n",
    "    clf = svm(kernel='linear', C=1, maxiter=np.float('Inf'), gamma=1)\n",
    "    w,b = clf.fit(X, y)\n",
    "    \n",
    "    clf.summary()\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        plt.plot(X[i,0],X[i,1],'.',c=(0.5+0.2*y[i],0,0.5-0.2*y[i]))\n",
    "    \n",
    "    x1s = np.linspace(-1,2,201)\n",
    "    x2s = -w[0]/w[1]*x1s - b/w[1]\n",
    "    plt.plot(x1s,x2s,'k-')\n",
    "    x2s = -w[0]/w[1]*x1s - (b+1)/w[1]\n",
    "    plt.plot(x1s,x2s,'k-')\n",
    "    x2s = -w[0]/w[1]*x1s - (b-1)/w[1]\n",
    "    plt.plot(x1s,x2s,'k-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
