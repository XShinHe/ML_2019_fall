{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设计一个显著减少支持向量数目，但没有明显下降泛化能力的SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的生成：\n",
    "\n",
    "不失一般性，我们考虑特征空间只有两个自由度的分类问题。 问题包含两个类别，一个类是在$(0,0)$为中心$\\sigma=0.5$为方差的二维正态分布产生的随机变量,记为负类$-1$；一个类是在$(1,1)$为中心$\\sigma$为方差的二维正态分布产生的随机变量，记为正类$+1$。数据集中含有1000个样本，明显，数据集不是完全可分的，而是近似可分的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的生成代码为：\n",
    "***\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "N=1000\n",
    "r = np.random.randint(2,size=N)\n",
    "X = np.zeros((N,2))\n",
    "y = np.zeros((N))\n",
    "X[:,0] = np.random.normal(r,0.5)\n",
    "X[:,1] = np.random.normal(r,0.5)\n",
    "y = 2*r-1\n",
    "```\n",
    "***\n",
    "存为csv文件, 之后每次都使用固定的数据集进行测试。\n",
    "\n",
    "> 注：助教推荐的数据集网站要么因为不符合二分类问题，要么因为样本数有省缺值或样本量太多或太少，不是很合适于这次方法策略的阐述。因此我们使用了自我启发所产生的数据集，即能否将服从两种不同分布的随机变量分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文献调研  \n",
    "\n",
    "* [Ho Gi Jung, and Gahyun Kim, Support Vector Number Reduction: Survey and Experimental \n",
    "Evaluations, IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL.15, NO.2, APRIL \n",
    "2014](https://ieeexplore.ieee.org/document/6623200)\n",
    "* [P. M. L. Drezet and R. F. Harrison, “A new method for sparsity control in support vector \n",
    "classification and regression,” Pattern Recognit., vol.34, no.1, pp. 111–125, Jan. 2001](\n",
    "https://www.sciencedirect.com/science/article/pii/S0031320399002034)\n",
    "* [O. Dekel and Y. Singer, “Support vector machines on a budget,” in Proc. NIPS Found., 2006, \n",
    "pp.1–8](https://ieeexplore.ieee.org/book/6267330)\n",
    "* [S.-Y. Chiu, L.-S. Lan, and Y.-C. Hwang, “Two sparsity-controlled schemes for 1-norm support \n",
    "vector classification,” in Proc. IEEE Northeast Workshop Circuits Syst., Montreal, QC, Canada, \n",
    "Aug. 5–8, 2007,pp. 337–340](https://ieeexplore.ieee.org/document/4487961?arnumber=4487961)\n",
    "\n",
    "我们主要阅读和参考了如上（但不仅仅包括如上）的一些文献。对于减少SVM中支持向量的方法都有了一定系统性的认识。\n",
    "\n",
    "首先，因该指出在SVM的二分类问题中，支持向量的个数是与训练集的大小呈线性关系的。\n",
    "\n",
    "而标准的SVM的计算复杂度为$\\mathcal{nn_{sv} + n_{sv}^3}$, 是与支持向量的三次方呈正比的。因此通过各种方法约化减少支持向量的数目。是改进SVM效率和性能的重要途径。\n",
    "\n",
    "针对约化支持向量数目的方法，主要分为两个派别，一个是预修剪(pre-pruning)，一个是后修剪(post-pruning)。\n",
    "\n",
    "* 预修剪中，我们在优化问题的同时，就考虑加上对于支持向量数目的惩罚（cost）或数目的约束(constraint)；或者，我们可以在分类前，对样本数据进行处理，如聚类(cluster)或随机抽样（Random）的方法，有效地减少优化问题中的支持向量的数目。\n",
    "\n",
    "* 后修剪，是建立在标准的SVM之后，即通过SVM的结果，抽取或构建出新的SVM分类问题，使得新的SVM分类问题中的支持向量数目得到极大的约化，同时很好地描述原有的SVM分类问题。后修剪方法，可以是从约化集合的选择、或约化集合的构建已得到最优的近似特征向量；也可以是对新的学习样例的选择，以达到构造的新SVM具有更少的支持向量的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "以下从三个方面，即带有对SV数目惩罚项的优化算法、对样本的预处理（Clsutering或Random Sampling）、后修剪三个方向，对支持向量数目的约化方法进行探讨和实践。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带有对SV数目惩罚项的算法\n",
    "\n",
    "***\n",
    "\n",
    "### 等效于优化对偶问题\n",
    "$$\n",
    "\\min_{\\alpha} \\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_i y_j K(x_i,x_j) - \\gamma \\sum_i \\alpha_i\n",
    "$$\n",
    "\n",
    "其中对于原始的SVM, $\\gamma=1$；而对于SV数目有惩罚的优化问题，$\\gamma <1$。\n",
    "\n",
    "这里的核函数可以选取为:\n",
    "\n",
    "* 线性核函数: $K(x_i,x_j) = x_i \\cdot x_j$\n",
    "> \n",
    "或者记矩阵$X_{i,.} = x_i$,那么核矩阵\n",
    "$$\n",
    "K = XX^T\n",
    "$$\n",
    "\n",
    "* 多项式核函数: $K(x_i,x_j) = (x_i \\cdot x_j +1)^p$\n",
    "* 高斯核函数: $K(x_i,x_j) = \\exp(-\\|x_i - x_j\\|^2/2\\sigma^2)$\n",
    "> \n",
    "对于高斯核的指数部分:\n",
    "$$\n",
    "\\|x_i - x_j\\|^2 = x_i\\cdot x_i +x_j\\cdot x_j - 2x_i\\cdot x_j \\\\\n",
    "= (NI^T + IN^T - 2XX^T)_{ij}\n",
    "$$\n",
    "其中$N_i = x_i\\cdot x_i$,$I_i = 1$,都是列向量.\n",
    "\n",
    "### SMO算法\n",
    "\n",
    "* 子问题的解决（minimial）\n",
    "\n",
    "一次只考虑两个变量的同时优化\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha_1,\\alpha-2} W(\\alpha_1,\\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \n",
    "\\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2 K_{12}\\alpha_1 \\alpha_2 - \\gamma (\\alpha_1+\\alpha_2) + y_1\\alpha_1\\sum_{i=3}^N y_i\\alpha_i K_{i1} + y_2\\alpha_2\\sum_{i=3}^N y_i\\alpha_i K_{i2} \\\\\n",
    "\\text{s.t. } \\alpha_1 y_1 + \\alpha_2 y_2 = \\zeta, 0 \\le \\alpha_1 \\le C, 0\\le \\alpha_2 \\le C\n",
    "$$\n",
    "\n",
    "记\n",
    "\n",
    "$$\n",
    "v_i = \\sum_{i=3}^N\\alpha_j y_iK(x_j,x_i) = g(x_i) - \\sum_{j=1}^2 \\alpha_j y_j K(x_j,x_i)-b,\\, i=1,2\n",
    "\\\\\n",
    "\\alpha_1 = y_1\\zeta - y_1y_2\\alpha_2\n",
    "$$\n",
    "\n",
    "则代入得到\n",
    "\n",
    "$$\n",
    "W = \\frac{1}{2} K_{11}\\alpha_1^2 + \\frac{1}{2} K_{22}\\alpha_2^2 + y_1y_2 K_{12}\\alpha_1\\alpha_2  - \\gamma (\\alpha_1 + \\gamma_2) + y_1 v_1 \\alpha_1 + y_2 v_2 \\alpha_2 \\\\\n",
    "= \\frac{1}{2} K_{11} (\\zeta-y_2\\alpha_2)^2 + \\frac{1}{2} K_{22}\\alpha_2^2 + y_2 K_{12}(\\zeta-y_2\\alpha_2)\\alpha_2  - \\gamma (y_1(\\zeta-y_2\\alpha_2) + \\alpha_2) + v_1 (\\zeta-y_2\\alpha_2) + y_2 v_2 \\alpha_2 \\\\\n",
    "$$\n",
    "\n",
    "将$W$对$\\alpha_2$求导数\n",
    "$$\n",
    "\\partial_{\\alpha_2}W = K_{11}\\alpha_2 + K_{22}\\alpha_2 - 2K_{12}\\alpha_2 - K_{11}\\zeta y_2 + K_{12}\\zeta y_2 + \\gamma (y_1y_2-1) - v_1 y_2 + v_2y_2 = 0\n",
    "$$\n",
    "得到\n",
    "$$\n",
    "(K_{11}+K_{22}-2K_{12}) \\alpha_2 = y_2(\\gamma (y_2-y_1) + \\zeta K_{11} - \\zeta K_{12} + v_1 - v_2)\n",
    " = (K_{11}+K_{22}-2K_{12}) \\alpha_2^{old} + y_2(\\tilde{E}_1-\\tilde{E}_2)\n",
    "$$\n",
    "\n",
    "这里\n",
    "$$\n",
    "\\tilde{E}_i = g(x_i) - \\gamma y_i = E_i + (1-\\gamma)y_i\\\\\n",
    "E_i = g(x_i) - y_i\n",
    "$$\n",
    "\n",
    "* 如何选择子问题（sequential）\n",
    "\n",
    "> * 第一个变量i,\n",
    ">> * 首先选择间隔边界上的支持向量点,选择最违反KKT条件的点;\n",
    ">> * 如果都满足KKT条件,遍历整个训练集,选择最违反KKT的点.\n",
    ">* 第二个变量j\n",
    ">> * 选择$|E_i-E_j|$最大;\n",
    ">> * 如果没有太大下降,则选择边界上的支持向量点;\n",
    ">> * 如果还没有足够下降,则遍历整个训练集;\n",
    ">> * 如果还没有,则在外层循环寻找下一个合适的i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对训练样本数据的预处理方法\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering 抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random 抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后修剪方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习样例的选择\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 约化集的构建（post-construct）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
