{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设计一个显著减少支持向量数目，但没有明显下降泛化能力的SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的生成：\n",
    "\n",
    "不失一般性，我们考虑特征空间只有两个自由度的分类问题。 问题包含两个类别，一个类是在$(0,0)$为中心$\\sigma=0.5$为方差的二维正态分布产生的随机变量,记为负类$-1$；一个类是在$(1,1)$为中心$\\sigma$为方差的二维正态分布产生的随机变量，记为正类$+1$。数据集中含有1000个样本，明显，数据集不是完全可分的，而是近似可分的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的生成代码为：\n",
    "***\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "N=1000\n",
    "r = np.random.randint(2,size=N)\n",
    "X = np.zeros((N,2))\n",
    "y = np.zeros((N))\n",
    "X[:,0] = np.random.normal(r,0.5)\n",
    "X[:,1] = np.random.normal(r,0.5)\n",
    "y = 2*r-1\n",
    "```\n",
    "***\n",
    "存为csv文件, 之后每次都使用固定的数据集进行测试。\n",
    "\n",
    "> 注：助教推荐的数据集网站要么因为不符合二分类问题，要么因为样本数有省缺值或样本量太多或太少，不是很合适于这次方法策略的阐述。因此我们使用了自我启发所产生的数据集，即能否将服从两种不同分布的随机变量分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始的SVM程序  \n",
    "\n",
    "数据集 V = {$(x_i,y_i)$}$^N_{i=1}$,$y_i \\in ${1,-1},$x_i \\in R^n$是一个二分类问题\n",
    "\n",
    "目标是找一个超平面$$w \\cdot x +b$$\n",
    "使得对于正类有$$w \\cdot x +b>0$$ \n",
    "对于负类有$$w \\cdot x +b<0$$\n",
    "误分类点$(x_i,y_i)$有$$y_i \\dot (w \\cdot x_i +b)<0$$\n",
    "使误分类点个数最少等价于函数间隔最小即$$\\gamma = min_{i=1,2,\\cdots,N} y_i \\dot (w \\cdot x_i +b)$$\n",
    "现要找使得最大间隔分离超平面$max \\frac{\\gamma}{||w||}$使得$y_i \\dot (\\frac{w}{||w||} \\cdot x_i + \\frac{b}{||w||})\\geq \\frac{\\gamma}{||w||}$,i = 1,2$\\cdots$N  \n",
    "等价于 min$\\frac{1}{2} \\dot {||w||}^2$使得$y_i \\dot (w \\cdot x_i +b)-1\\geq0$,i = 1,2$\\cdots$N  \n",
    "现引入Lagrange函数,$$L(w,b,\\alpha) = \\frac{1}{2} \\dot {||w||}^2 - \\sum ^N_{i=1} \\alpha_i\\dot y_i\\dot (w \\cdot x_i +b)+\\sum ^N_{i=1} \\alpha_i$$\n",
    "求解其对偶问题$$max_\\alpha min_{w,b}L(w,b,\\alpha)$$\n",
    "$$y_i \\dot (w \\cdot x_i +b)-1\\geq 0$$  \n",
    "求解$min_{w,b}L(w,b,\\alpha)$,将Lagrange分别对w,b求偏导数并令其等于0，得  \n",
    "$$w = \\sum ^N_{i=1} \\alpha_i\\dot y_i\\dot x_i$$\n",
    "$$\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$  \n",
    "回带即得$$min_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)+\\sum ^N_{i=1} \\alpha_i$$  \n",
    "即求得等价对偶问题$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i$$  \n",
    "$$\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$ \n",
    "$$\\alpha_i\\geq0,i=1,2\\cdots,N$$\n",
    "满足KKT条件，记对偶问题最优解$(\\alpha^*,w^*,b^*)$，则有:  \n",
    "$$\\alpha^*_i\\dot y_i \\dot (w \\cdot x_i +b)-1 = 0,i = 1,2\\cdots,N$$\n",
    "$$y_i \\dot (w \\cdot x_i +b)-1\\geq0,i = 1,2\\cdots,N$$\n",
    "$$\\alpha^*_i\\geq0,i = 1,2\\cdots,N$$\n",
    "$\\Rightarrow \\alpha^*_i = 0,y_i \\dot (w \\cdot x_i +b)-1 > 0$，此时$\\alpha^*_i$不为支持向量  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha^*_i > 0,y_i \\dot (w \\cdot x_i +b)-1 = 0$，此时$\\alpha^*_i$为支持向量   \n",
    "所以支持向量的个数即$\\alpha^*_i > 0$的个数，为了有效减少支持向量的个数，可对对偶问题进行优化，使得$=\\alpha^*_i > 0$的个数减少.故在等价对偶问题对其加入惩罚项$B\\dot\\sum ^N_{i=1} \\alpha_i$,其中的B非负，得$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i+B\\dot\\sum ^N_{i=1} \\alpha_i$$  约化得:$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)+(B-1)\\dot\\sum ^N_{i=1} \\alpha_i$$  \n",
    "现实生活中训练数据线性可分的情形较少，训练数据往往是近似线性可分的，此时需要引入松弛变量$\\xi_i$及惩罚因子C>0，使其可分，此时原始最优化问题变为$$min_{w,b,\\xi}\\frac{1}{2} \\dot {||w||}^2+C\\dot\\sum^N_{i=1}\\xi_i$$使得$$y_i \\dot (w \\cdot x_i +b)\\geq 1-\\xi_i,i=1,2\\cdots,N$$$$\\xi_i\\geq0,i=1,2\\cdots,N$$  \n",
    "求解其等价对偶问题$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i$$使得$$\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$$$0\\leq\\alpha_i\\leq C,i = 1,2\\cdots N$$将其解$\\alpha^*$中$\\alpha^*\\>0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量，所以支持向量的个数即$\\alpha^*_i > 0$的个数。\n",
    "\n",
    "我们的合作小组构建了自己的SVM程序，详细可以参考在[ML小组合作项目Github](https://github.com/XShinHe/ML_2019_fall)。实现了SMO算法和数据显示分析的一些基本功能。（同时，对于SVM的优化算法，也后续地写在了里面）。\n",
    "下面是python程序的示例。（调用我们的ML程序的模块是使用`mlcore`）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlcore.svm as svm\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./dat/svm_Train.csv', header=None)\n",
    "X = df.values[:,0:2]\n",
    "y = df.values[:,-1]\n",
    "#print(df)\n",
    "\n",
    "clf = svm.svm(kernel='linear',C=1,maxiter=np.float('Inf'))\n",
    "clf.fit(X,y)\n",
    "clf.summary()\n",
    "clf.plot_surface(-1,2)\n",
    "plt.savefig('./dat/demo.png')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "accuracy is 0.922000\n",
    "\n",
    "Hyper surface:\n",
    "\tw =\n",
    " [ 96.0378537  104.01353933] \n",
    "\tb =\n",
    " -2.4166672623882426\n",
    "\n",
    "Support vector number:\t199\n",
    "```\n",
    "***\n",
    "![img](./dat/demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文献调研 -- 关于减少支持向量数目的策略\n",
    "\n",
    "* [Ho Gi Jung, and Gahyun Kim, Support Vector Number Reduction: Survey and Experimental \n",
    "Evaluations, IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL.15, NO.2, APRIL \n",
    "2014](https://ieeexplore.ieee.org/document/6623200)\n",
    "* [P. M. L. Drezet and R. F. Harrison, “A new method for sparsity control in support vector \n",
    "classification and regression,” Pattern Recognit., vol.34, no.1, pp. 111–125, Jan. 2001](\n",
    "https://www.sciencedirect.com/science/article/pii/S0031320399002034)\n",
    "* [O. Dekel and Y. Singer, “Support vector machines on a budget,” in Proc. NIPS Found., 2006, \n",
    "pp.1–8](https://ieeexplore.ieee.org/book/6267330)\n",
    "* [S.-Y. Chiu, L.-S. Lan, and Y.-C. Hwang, “Two sparsity-controlled schemes for 1-norm support \n",
    "vector classification,” in Proc. IEEE Northeast Workshop Circuits Syst., Montreal, QC, Canada, \n",
    "Aug. 5–8, 2007,pp. 337–340](https://ieeexplore.ieee.org/document/4487961?arnumber=4487961)\n",
    "\n",
    "我们主要阅读和参考了如上（但不仅仅包括如上）的一些文献。对于减少SVM中支持向量的方法都有了一定系统性的认识。\n",
    "\n",
    "首先，因该指出在SVM的二分类问题中，支持向量的个数是与训练集的大小呈线性关系的。\n",
    "\n",
    "而标准的SVM的计算复杂度为$\\mathcal{nn_{sv} + n_{sv}^3}$, 是与支持向量的三次方呈正比的。因此通过各种方法约化减少支持向量的数目。是改进SVM效率和性能的重要途径。\n",
    "\n",
    "针对约化支持向量数目的方法，主要分为两个派别，一个是预修剪(pre-pruning)，一个是后修剪(post-pruning)。\n",
    "\n",
    "* 预修剪中，我们在优化问题的同时，就考虑加上对于支持向量数目的惩罚（cost）或数目的约束(constraint)；或者，我们可以在分类前，对样本数据进行处理，如聚类(cluster)或随机抽样（Random）的方法，有效地减少优化问题中的支持向量的数目。\n",
    "\n",
    "* 后修剪，是建立在标准的SVM之后，即通过SVM的结果，抽取或构建出新的SVM分类问题，使得新的SVM分类问题中的支持向量数目得到极大的约化，同时很好地描述原有的SVM分类问题。后修剪方法，可以是从约化集合的选择、或约化集合的构建已得到最优的近似特征向量；也可以是对新的学习样例的选择，以达到构造的新SVM具有更少的支持向量的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本文策略**：以下从三个方面，即带有对SV数目惩罚项的优化算法、对样本的预处理（Clsutering或Random Sampling）、后修剪三个方向，对支持向量数目的约化方法进行探讨和实践。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带有对SV数目惩罚项的算法\n",
    "\n",
    "***\n",
    "\n",
    "### 等效于优化对偶问题\n",
    "\n",
    "同理为了有效减少支持向量的个数，可对对偶问题进行优化，使得$=\\alpha^*_i > 0$的个数减少。故在等价对偶问题对其加入惩罚项$B\\dot\\sum ^N_{i=1} \\alpha_i$,其中的B非负，得$$min_\\alpha \\frac{1}{2}\\sum^N_{i=1}\\sum^N_{j=1}\\alpha_i\\dot\\alpha_j\\dot y_i\\dot y_j\\dot(x_i\\cdot x_j)-\\sum ^N_{i=1} \\alpha_i+B\\dot\\sum ^N_{i=1} \\alpha_i$$  \n",
    "\n",
    "或\n",
    "$$\n",
    "\\min_{\\alpha} \\frac{1}{2}\\sum_i\\sum_j \\alpha_i\\alpha_j y_i y_j K(x_i,x_j) - \\gamma \\sum_i \\alpha_i\n",
    "$$\n",
    "\n",
    "使得\n",
    "$$\n",
    "\\sum ^N_{i=1} \\alpha_i\\dot y_i = 0$$$$0\\leq\\alpha_i\\leq C,i = 1,2\\cdots N\n",
    "$$\n",
    "\n",
    "其中对于原始的SVM, $\\gamma=1$；而对于SV数目有惩罚的优化问题，$\\gamma <1$。\n",
    "\n",
    "这里的核函数可以选取为:\n",
    "\n",
    "* 线性核函数: $K(x_i,x_j) = x_i \\cdot x_j$\n",
    "> \n",
    "或者记矩阵$X_{i,.} = x_i$,那么核矩阵\n",
    "$$\n",
    "K = XX^T\n",
    "$$\n",
    "\n",
    "* 多项式核函数: $K(x_i,x_j) = (x_i \\cdot x_j +1)^p$\n",
    "* 高斯核函数: $K(x_i,x_j) = \\exp(-\\|x_i - x_j\\|^2/2\\sigma^2)$\n",
    "> \n",
    "对于高斯核的指数部分:\n",
    "$$\n",
    "\\|x_i - x_j\\|^2 = x_i\\cdot x_i +x_j\\cdot x_j - 2x_i\\cdot x_j \\\\\n",
    "= (NI^T + IN^T - 2XX^T)_{ij}\n",
    "$$\n",
    "其中$N_i = x_i\\cdot x_i$,$I_i = 1$,都是列向量.\n",
    "\n",
    "### SMO算法\n",
    "\n",
    "* 子问题的解决（minimial）\n",
    "\n",
    "一次只考虑两个变量的同时优化\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha_1,\\alpha-2} W(\\alpha_1,\\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \n",
    "\\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2 K_{12}\\alpha_1 \\alpha_2 - \\gamma (\\alpha_1+\\alpha_2) + y_1\\alpha_1\\sum_{i=3}^N y_i\\alpha_i K_{i1} + y_2\\alpha_2\\sum_{i=3}^N y_i\\alpha_i K_{i2} \\\\\n",
    "\\text{s.t. } \\alpha_1 y_1 + \\alpha_2 y_2 = \\zeta, 0 \\le \\alpha_1 \\le C, 0\\le \\alpha_2 \\le C\n",
    "$$\n",
    "\n",
    "记\n",
    "\n",
    "$$\n",
    "v_i = \\sum_{i=3}^N\\alpha_j y_iK(x_j,x_i) = g(x_i) - \\sum_{j=1}^2 \\alpha_j y_j K(x_j,x_i)-b,\\, i=1,2\n",
    "\\\\\n",
    "\\alpha_1 = y_1\\zeta - y_1y_2\\alpha_2\n",
    "$$\n",
    "\n",
    "则代入得到\n",
    "\n",
    "$$\n",
    "W = \\frac{1}{2} K_{11}\\alpha_1^2 + \\frac{1}{2} K_{22}\\alpha_2^2 + y_1y_2 K_{12}\\alpha_1\\alpha_2  - \\gamma (\\alpha_1 + \\gamma_2) + y_1 v_1 \\alpha_1 + y_2 v_2 \\alpha_2 \\\\\n",
    "= \\frac{1}{2} K_{11} (\\zeta-y_2\\alpha_2)^2 + \\frac{1}{2} K_{22}\\alpha_2^2 + y_2 K_{12}(\\zeta-y_2\\alpha_2)\\alpha_2  - \\gamma (y_1(\\zeta-y_2\\alpha_2) + \\alpha_2) + v_1 (\\zeta-y_2\\alpha_2) + y_2 v_2 \\alpha_2 \\\\\n",
    "$$\n",
    "\n",
    "将$W$对$\\alpha_2$求导数\n",
    "$$\n",
    "\\partial_{\\alpha_2}W = K_{11}\\alpha_2 + K_{22}\\alpha_2 - 2K_{12}\\alpha_2 - K_{11}\\zeta y_2 + K_{12}\\zeta y_2 + \\gamma (y_1y_2-1) - v_1 y_2 + v_2y_2 = 0\n",
    "$$\n",
    "得到\n",
    "$$\n",
    "(K_{11}+K_{22}-2K_{12}) \\alpha_2 = y_2(\\gamma (y_2-y_1) + \\zeta K_{11} - \\zeta K_{12} + v_1 - v_2)\n",
    " = (K_{11}+K_{22}-2K_{12}) \\alpha_2^{old} + y_2(\\tilde{E}_1-\\tilde{E}_2)\n",
    "$$\n",
    "\n",
    "这里\n",
    "$$\n",
    "\\tilde{E}_i = g(x_i) - \\gamma y_i = E_i + (1-\\gamma)y_i\\\\\n",
    "E_i = g(x_i) - y_i\n",
    "$$\n",
    "\n",
    "* 如何选择子问题（sequential）\n",
    "\n",
    "> * 第一个变量i,\n",
    ">> * 首先选择间隔边界上的支持向量点,选择最违反KKT条件的点;\n",
    ">> * 如果都满足KKT条件,遍历整个训练集,选择最违反KKT的点.\n",
    ">* 第二个变量j\n",
    ">> * 选择$|E_i-E_j|$最大;\n",
    ">> * 如果没有太大下降,则选择边界上的支持向量点;\n",
    ">> * 如果还没有足够下降,则遍历整个训练集;\n",
    ">> * 如果还没有,则在外层循环寻找下一个合适的i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4565caf7f6e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gauss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernelargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sigma'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mnumsv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0maccur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hexin/CodeNote/Code/ML_2019_fall/mlcore/svm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m#print(self.predict, self.error, self.kktbias, self.kktbias_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSMO_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hexin/CodeNote/Code/ML_2019_fall/mlcore/svm.py\u001b[0m in \u001b[0;36mSMO_sequential\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merr_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                     \u001b[0mchanged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSMO_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mchanged\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hexin/CodeNote/Code/ML_2019_fall/mlcore/svm.py\u001b[0m in \u001b[0;36mSMO_minimal\u001b[0;34m(self, i, j)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# solve new aj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0meta\u001b[0m \u001b[0;31m#《statistic learning method》p127(7.106)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#《statistic learning method》p127(7.108)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlcore.svm as svm\n",
    "\n",
    "df = pd.read_csv('./dat/svm_Train.csv', header=None)\n",
    "#print(df)\n",
    "\n",
    "X = df.values[:,0:2]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "N = 4\n",
    "gammas = np.linspace(1,0.7,N)\n",
    "numsv = np.zeros((N))\n",
    "accur = np.zeros((N))\n",
    "\n",
    "for i in range(N):\n",
    "    clf = svm.svm(kernel='gauss', kernelargs={'sigma':1}, gamma=gammas[i], C=1, maxiter=np.float('Inf'))\n",
    "    clf.fit(X,y)\n",
    "    numsv[i] = clf.nsv\n",
    "    accur[i] = clf.accuracy\n",
    "    \n",
    "plt.plot(numsv, accur, 'r.-', label='penalty')\n",
    "plt.plot(numsv, np.ones((N))*accur[0], 'k--', label='full Train')\n",
    "plt.xlabel('number of SV')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim((0.5,1))\n",
    "plt.savefig('./dat/penalty.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练样本选择的选择预处理\n",
    "\n",
    "### 思路\n",
    "原始的SVM问题往往需要用到大量的支持向量，随着训练样本数的增多，直观上我们需要的支持向量的数量也在增多，因而一个直观的降低支持向量的方法就是利用某些方式减少训练样本的数量。随着样本数量的减少，我们不仅可以减少所需的支持向量的数量，同时在一定程度上我们也可以将样本中的噪声消除。\n",
    "\n",
    "### 方法\n",
    "在具体的实现中，我们主要提供了两种方式来降低样本的数量。\n",
    "- **随机抽样**\n",
    "    这种方式最简单，但是由于随机抽样获得的样本未必可以很好地反映整个数据集，所以可能对最终模型的泛化行产生一定的影响\n",
    "- **聚类**\n",
    "    利用聚类进行样本的选择更为直观，我们将空间上更相近的点归并成为一个点，让归并的所有点投票决定这个归并点的类别，具体的思想类似于决策树的想法，这样的一个好处是可以让获得的样本点更具有代表性，直观上对模型的泛化性有一定的帮助。在这里我们利用了四种较为常用的聚类方式，分别是K均值、均值漂移、DBSCAN以及高斯混合模型。\n",
    "    - **K均值（K-Means**\n",
    "        K均值是最常用的一种聚类方式，我们需要预先提供簇的数量，K均值可以很好地将各个点划分到各个簇中。在具体地设计中，我们首先利用K均值进行类别的划分，接下来确定每个簇y值之和，如果y值之和是0，那么我们认为这个簇无法确定其类别，我们就抛弃这个簇，同时将原来的所有点加回样本集中，如果y值之和不是0，那么我们就将这个簇的中心加入样本集中，同时用y值之和表示其类别（y值判断）。\n",
    "    - **均值漂移（Mean-shift**\n",
    "        均值漂移不需要预先提供簇的数量，它寻找核密度极值点并作为簇的质心，然后根据最近邻原则为样本点赋予质心，这解决了K均值面临的需要预先给出初始质心的缺点。我们首先利用均值漂移进行簇的划分，然后同理，我们还是计算y值之和，如果y值之和为0，则将簇内所有点加入样本集中，反之将簇中心加入样本集中。\n",
    "    - **DBSCAN**\n",
    "        DBSCAN也不需要预先提供簇的数量，需要提供$\\epsilon$邻域的距离阈值以及样本数阈值，它的优点是可发现任意形状的簇类，同时可以识别出来噪声点。我们首先利用DBSCAN进行簇的划分，然后同理进行簇的y值判断，由于DBSCAN会识别出噪声点，所以我们最后将噪声点加入样本集中。\n",
    "    - **高斯混合模型（Gaussian Mixtures Model**\n",
    "        高斯混合模型是将事物分解为若干的基于高斯概率密度函数形成的模型。我们首先利用高斯混合模型进行划分，然后同上进行y值判断。\n",
    "\n",
    "### 性能分析\n",
    "**随机抽样** \n",
    "\n",
    "|比例|支持向量数量|训练集准确率|测试集准确率|\n",
    "|:------:|:------:|:------:|:------:|\n",
    "|1|199|0.922|0.924|\n",
    "|0.1|28|0.920|0.914|\n",
    "|0.2|51|0.920|0.922|\n",
    "|0.3|68|0.927|0.924|\n",
    "\n",
    "**K均值**\n",
    "\n",
    "|簇|支持向量数量|训练集准确率|测试集准确率|\n",
    "|:------:|:------:|:------:|:------:|\n",
    "|原始|199|0.922|0.924|\n",
    "|500|139|0.896|0.924|\n",
    "|300|83|0.909|0.924|\n",
    "|200|59|0.905|0.918|\n",
    "\n",
    "**均值漂移**\n",
    "\n",
    "|Bandwith|支持向量数量|训练集准确率|测试集准确率|\n",
    "|:------:|:------:|:------:|:------:|\n",
    "|原始|199|0.922|0.924|\n",
    "|0.1|50|0.941|0.926|\n",
    "|0.2|29|0.917|0.924|\n",
    "|0.3|50|0.765|0.926|\n",
    "\n",
    "**DBSCAN**\n",
    "\n",
    "|eps|min_sample|支持向量数量|训练集准确率|测试集准确率|\n",
    "|:------:|:------:|:------:|:------:|:------:|\n",
    "|原始|原始|199|0.922|0.924|\n",
    "|0.1|10|137|0.893|0.924|\n",
    "|0.1|20|199|0.922|0.924|\n",
    "|0.2|10|17|0.936|0.916|\n",
    "\n",
    "**高斯混合模型**\n",
    "\n",
    "|component|支持向量数量|训练集准确率|测试集准确率|\n",
    "|:------:|:------:|:------:|:------:|\n",
    "|原始|199|0.922|0.924|\n",
    "|500|142|0.891|0.924|\n",
    "|300|81|0.907|0.924|\n",
    "|100|29|0.926|0.920|\n",
    "\n",
    "### 结论\n",
    "通过上述实验结果可以发现，使用随机抽样和聚类的方式都可以显著降低支持向量的数量。在随机抽样中我们发现，随着抽样比例的增加，支持向量数越来越多，在测试集上的准确率逐渐上升，有时效果超过了原始的准确率；在聚类方法中，我们发现随着类数量的减少，支持向量的数量也随着减少，在训练集上的准确率以及测试集上的效果基本维持不变，即泛化性基本不受影响。不过Meanshift和DBCAN方法在部分参数下时间会较长，这受到二者具体运行时影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后修剪方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习样例的选择\n",
    "\n",
    "采用SCA（seperable case approximation）或者SSCA（smoothed SCA）策略，将错误分类点，或太接近分离超平面的点都舍弃。剩下的点在原则上，能给出较少的支持向量数目！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlcore.svm as svm\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./dat/svm_Train.csv', header=None)\n",
    "X = df.values[:,0:2]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "#print(df)\n",
    "\n",
    "clf = svm.svm(kernel='linear',C=1,maxiter=np.float('Inf'))\n",
    "postclf = svm.svm(kernel='linear',C=1,maxiter=np.float('Inf'))\n",
    "clf.fit(X,y)\n",
    "accur_ori = clf.accuracy\n",
    "\n",
    "Nplot = 6\n",
    "D = np.linspace(0,0.5,Nplot)\n",
    "\n",
    "numsv = np.zeros((Nplot+1))\n",
    "accur = np.zeros((Nplot+1))\n",
    "numsv[0] = clf.nsv\n",
    "accur[0] = clf.accuracy\n",
    "for i in range(Nplot):\n",
    "    Xs, ys = clf.PostSample(D[i])\n",
    "    postclf.fit(Xs,ys)\n",
    "    numsv[i+1] = postclf.nsv\n",
    "    accur[i+1] = postclf.test_accuracy(X,y)\n",
    "    \n",
    "    #print(D[i], numsv[i+1], accur[i+1])\n",
    "    \n",
    "plt.plot(numsv, accur, '.-', c='r', label='SSCA test')\n",
    "plt.plot(numsv, np.ones((Nplot+1))*accur_ori, '--', c='k', label='full train')\n",
    "plt.xlabel('number of SV')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim((0.5,1))\n",
    "plt.legend(loc=0)\n",
    "plt.savefig('./dat/SSCA.png')\n",
    "plt.show()\n",
    "```\n",
    "***\n",
    "![SSCA](./dat/SSCA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述测试中，SCA对应$D=0$，将错分类点完全删去；而$D\\ne 0$时，即考虑将$y_i(w\\cdot x_i+b)>D$的点完全删去。 可见，我们测试了D={0,0.1,0.2,0.3,0.4,0.5}的情况，构造出新的SVM，并测试约化SVM模型对于全数据集的准确度。我们发现，随着D的增大，支持向量（SV）的数目明显变少，同时训练出的模型对于全数据集的精确度没有明显下降。这说明了SSCA是一种好的后修剪减少支持向量数目的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 约化集的构建（post-construct）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们记$\\tilde{\\alpha}_i=y_i \\alpha_i$，则原始的SVM优化得到的特征向量为$w=\\sum_i \\tilde{\\alpha_i}\\Phi(x_i), i=1,\\cdots,l$,我们假定另一组样本空间给出的$w'=\\sum_k \\tilde{\\beta}_k \\Phi(z_k), k=1,\\cdots,m$，其中$m<l$能够近似给出$w\\approx w'$。那么问题的关键就在于如何构造出一组$z_k$和$\\tilde{\\beta}_k$。首先给定$z_k$，是可以求出最有的$\\beta_k$的：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\tilde{\\beta}_k} \\|w-w'\\|^2 = 0\n",
    "\\\\\n",
    "\\to \\tilde{\\beta} = (K_{zz})^{-1} K_{zx} \\tilde{\\alpha}\n",
    "$$\n",
    "这里$(K_{zz})_{ij}=k(z_i,z_j)$，$(K_{zx})_{ij}=k(z_i,x_j)$\n",
    "\n",
    "IPA的想法是每找到一个$z_{m-1}$，新构造的特征向量为$w_m = \\sum_i^l\\tilde{\\alpha}_i \\Phi(x)i) - \\sum_{i=1}^{m-1}\\tilde{\\beta}_i\\Phi(z_i)$，通过找$w_m$的最优原象$z_m$作为第m个新构造的样本，直到达到满足的数目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原象通过优化拿到\n",
    "$$\n",
    "\\hat{z} = \\arg\\min_{z} \\|\\Phi(z) - \\sum_i\\tilde{\\gamma}_i\\Phi(\\chi_i) \\|^2\n",
    "$$\n",
    "其中$\\tilde{\\gamma} = \\{\\tilde{\\alpha}, -\\tilde{\\beta}_1, \\cdots, -\\tilde{\\beta}_{m-1}\\}$，而$\\chi=\\{x, z_1, \\cdots, z_{m-1}\\}$\n",
    "\n",
    "特别地，对于RBF类型的SVM，这等效于优化\n",
    "\n",
    "$$\n",
    "\\hat{z} = \\arg\\max_{z} \\sum_i \\tilde{\\alpha}_i k(z,x_i)\n",
    "$$\n",
    "\n",
    "可以采用梯度下降法进行优化得到$z_m=\\hat{z}$。\n",
    "每得到一个$z_m$，需要进一步更新全部的$\\tilde{\\beta}_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4697100417723314 -0.957921689120327 -7.51593172078263e-05\n",
      "done Z once\n",
      "-7.515942304771699e-05 -7.526167218533625e-05 -7.542738952924643e-05\n",
      "done Z once\n",
      "-7.499424917150538e-05 -7.509607422676175e-05 -7.526110371390839e-05\n",
      "done Z once\n",
      "-7.482975982475829e-05 -7.493116337374175e-05 -7.509550918576077e-05\n",
      "done Z once\n",
      "-7.466595085253579e-05 -7.47669354507576e-05 -7.493060173585503e-05\n",
      "done Z once\n",
      "-7.450281813305811e-05 -7.460338631565474e-05 -7.476637718893843e-05\n",
      "done Z once\n",
      "-7.434035757737736e-05 -7.444051185931207e-05 -7.4602831403118e-05\n",
      "done Z once\n",
      "-7.417856512907824e-05 -7.427830800533957e-05 -7.443996026955568e-05\n",
      "done Z once\n",
      "-7.401743676391416e-05 -7.411677070971237e-05 -7.42777597120965e-05\n",
      "done Z once\n",
      "-7.385696848952991e-05 -7.395589596049039e-05 -7.411622568698857e-05\n",
      "done Z once\n",
      "-7.369715634513182e-05 -7.37956797774882e-05 -7.395535418254482e-05\n",
      "done Z once\n",
      "-7.353799640117695e-05 -7.363611821196017e-05 -7.379514121882452e-05\n",
      "done Z once\n",
      "-7.337948475907887e-05 -7.347720734630412e-05 -7.363558284733913e-05\n",
      "done Z once\n",
      "-7.322161755089434e-05 -7.331894329374676e-05 -7.347667515072665e-05\n",
      "done Z once\n",
      "-7.306439093902435e-05 -7.316132219804331e-05 -7.331841424245335e-05\n",
      "done Z once\n",
      "-7.290780111593199e-05 -7.30043402331896e-05 -7.316079626651886e-05\n",
      "done Z once\n",
      "-7.275184430384905e-05 -7.28479936031311e-05 -7.30038173971637e-05\n",
      "done Z once\n",
      "-7.259651675446445e-05 -7.269227854144623e-05 -7.284747383855002e-05\n",
      "done Z once\n",
      "-7.244181474866906e-05 -7.253719131108985e-05 -7.269176182449865e-05\n",
      "done Z once\n",
      "-7.228773459626155e-05 -7.238272820409876e-05 -7.253667761819461e-05\n",
      "done Z once\n",
      "-7.21342726356684e-05 -7.222888554130612e-05 -7.238221751189574e-05\n",
      "done Z once\n",
      "-7.198142523367233e-05 -7.207565967207358e-05 -7.222837782666304e-05\n",
      "done Z once\n",
      "-7.182918878514007e-05 -7.192304697401009e-05 -7.207515491207663e-05\n",
      "done Z once\n",
      "-7.16775597127517e-05 -7.177104385270536e-05 -7.192254514596612e-05\n",
      "done Z once\n",
      "-7.152653446673852e-05 -7.161964674146219e-05 -7.177054493413793e-05\n",
      "done Z once\n",
      "-7.137610952461089e-05 -7.146885210102409e-05 -7.161915071010193e-05\n",
      "done Z once\n",
      "-7.122628139091269e-05 -7.131865641932712e-05 -7.146835893482022e-05\n",
      "done Z once\n",
      "-7.107704659695459e-05 -7.116905621123273e-05 -7.131816609643498e-05\n",
      "done Z once\n",
      "-7.092840170056643e-05 -7.102004801827688e-05 -7.116856871001816e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae973cf74fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mNc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPostConstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hexin/CodeNote/Code/ML_2019_fall/mlcore/svm.py\u001b[0m in \u001b[0;36mPostConstruct\u001b[0;34m(self, Nc)\u001b[0m\n\u001b[1;32m    548\u001b[0m                 \u001b[0mNt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preimg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grad'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grad'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                     \u001b[0mXc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreimg_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preimg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grad'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fixed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                     \u001b[0mXc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreimg_fixedpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hexin/CodeNote/Code/ML_2019_fall/mlcore/svm.py\u001b[0m in \u001b[0;36mpreimg_gradient\u001b[0;34m(self, Xc, beta, Nt)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mZc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mZb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0mcosta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcostb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcostc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m#print(costa, costb, costc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hexin/CodeNote/Code/ML_2019_fall/mlcore/svm.py\u001b[0m in \u001b[0;36mcost\u001b[0;34m(Zt)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0mKz1xNt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKz1xNt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hexin/CodeNote/Code/ML_2019_fall/mlcore/svm.py\u001b[0m in \u001b[0;36m_Kernel\u001b[0;34m(self, X, Xp)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0m_N2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0m_Np2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mnorm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Np2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_N2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Ip\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2255\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlcore.svm as svm\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./dat/svm_Train.csv', header=None)\n",
    "X = df.values[:,0:2]\n",
    "y = df.values[:,-1]\n",
    "#print(df)\n",
    "\n",
    "clf = svm.svm(kernel='gauss', kernelargs={'sigma':1},C=1, maxiter=np.float('Inf'))\n",
    "postclf = svm.svm(kernel='gauss',kernelargs={'sigma':1},C=1, maxiter=np.float('Inf'))\n",
    "\"\"\"\n",
    "clf.fit(X,y)\n",
    "accur_ori = clf.accuracy\n",
    "print(clf.nsv, clf.accuracy)\n",
    "df_alpha = pd.DataFrame(clf.alpha)\n",
    "df_alpha.to_csv('./dat/rbf_alpha.csv', header=None, index=None)\n",
    "exit(-1)\n",
    "\"\"\"\n",
    "clf.X = X\n",
    "clf.y = y\n",
    "clf.Ns, clf.nfeature = np.shape(X)\n",
    "clf.alpha = pd.read_csv('./dat/rbf_alpha.csv', header=None).values[:,0]\n",
    "\n",
    "Nc = 5\n",
    "Xs,ys = clf.PostConstruct(Nc)\n",
    "print(Xs,ys)\n",
    "\n",
    "for i in range(Nc):\n",
    "    if ys[i] > 0:\n",
    "        plt.plot(Xs[i,0],Xs[i,1],'k.')\n",
    "    else:\n",
    "        plt.plot(Xs[i,0],Xs[i,1],'r.')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "rlist = range(10,Nc,10)\n",
    "nrlist = len(rlist)\n",
    "numsv = np.zeros((nrlist+1))\n",
    "accur = np.zeros((nrlist+1))\n",
    "for i in range(nrlist):\n",
    "    postclf.fit(Xs[0:i,:],ys[0:i])\n",
    "    numsv[i] = postclf.nsv\n",
    "    accur[i] = postclf.test_accuracy(X,y)\n",
    "    print(i,numsv[i], accur[i])\n",
    "\n",
    "numsv[-1] = 202\n",
    "accur[-1] = 0.922\n",
    "\n",
    "plt.plot(numsv, accur, '.-', c='r', label='IPA test')\n",
    "plt.plot(numsv, np.ones((Nplot+1))*accur_ori, '--', c='k', label='full train')\n",
    "plt.xlabel('number of SV')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim((0.5,1))\n",
    "plt.legend(loc=0)\n",
    "plt.savefig('./dat/IPA.png')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
